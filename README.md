# Conduit

This project implements an **event-driven outbox pattern** using Kafka, Debezium, Kafka Streams, and Schema Registry to propagate state changes from a PostgreSQL-based microservice into a shared, normalized event stream that can be consumed by downstream services (e.g., shipment service, S3 sinks, etc.).

## âœ¨ Why Conduit?
- **Decouple producers from consumers** â€” expose a stable, versioned public contract.
- **Normalize change events** â€” transform raw CDC JSON into Avro records.
- **Guarantee compatibility** â€” enforce schemas via Confluent Schema Registry.
- **Polyglot friendly** â€” consumers only need the Avro schema, not internal DB details.

## ğŸ— Architecture Overview
<img src="docs/architecture.png" width="500">

- **Debezium** monitors changes in the outbox table (`outboxevent`) via logical decoding.
- **Kafka Streams** normalizes and transforms the private JSON-based stream to a public Avro-based stream.
- **Schema Registry** ensures schema evolution and validation.
- **Consumers** (e.g., shipment-service) deserialize strongly typed Avro messages.



## ğŸ”§ Modules

| Module              | Description |
|---------------------|-------------|
| `order-service`     | Produces domain events into the `outboxevent` table (CDC source). |
| `debezium`          | PostgreSQL connector with logical decoding. |
| `kafka-streams`     | Reads from private JSON topic, normalizes into typed Avro records. |
| `shipment-service`  | Sample consumer of the public outbox stream using Specific Avro. |



## ğŸ§  Key Concepts

### ğŸ” Outbox Pattern
- Ensures reliable propagation of domain events by writing to an outbox table as part of the local transaction.
- Eliminates dual-write problems between DB and Kafka.

### ğŸ§¬ Schema Evolution with Avro
- Versioned topics (`orders.public.outbox.v1`) separate schema contracts.
- Avro ensures serialization compatibility with enforced schema types.

### ğŸ” Change Data Capture (CDC)
- Debezium reads logical changes using PostgreSQL's logical replication.
- Handles special PostgreSQL cases like TOAST columns:
    - See [Handling Unchanged TOAST Values](https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/)

### ğŸ“¦ Event Routing and Transformation
- Debezium connector transforms the CDC stream:
    - `ExtractNewRecordState` to unwrap payloads
    - `ByLogicalTableRouter` to rename topics
- Kafka Streams performs final enrichment and emits versioned Avro events.



## ğŸš¦ Key Behaviors

- **Null or `__debezium_unavailable_value` fields** are filtered to prevent deserialization failures.
- **Consumers** can independently subscribe to public topics without breaking changes via Avro + topic versioning.
- **Tombstones** can be handled if `tombstones.on.delete=true`, though dropped by default.



## ğŸ§ª Running Locally

```bash
make up
```

Then, interact via:

- `localhost:8080` â†’ Order service API
- `localhost:8091` â†’ Shipment service consumer logs

## ğŸ› ï¸ Generating Avro POJOs

To generate Java POJOs from your Avro schema files, run:

```bash
mvn generate-sources
```


## ğŸ“š References

- [The Schema Registry API is *Not* How You Use Schema with Kafka](https://medium.com/google-cloud/the-schema-registry-api-is-not-how-you-use-schema-with-kafka-3adb92f09764)
- [Is Kafka a Database (with ksqlDB)?](https://www.confluent.io/blog/is-kafka-a-database-with-ksqldb/)
- [Database Inside Out â€“ Martin Kleppmann](https://martin.kleppmann.com/2015/11/05/database-inside-out-at-oredev.html)
- [Handling Unchanged Postgres TOAST Values](https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/)
- [Debezium PostgreSQL Connector Docs](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)



## ğŸ“ Future Enhancements

- Add schema validation and metrics.
- Implement event reprocessing and dead-letter handling.
- Enable S3 archival via Kafka Connect + LocalStack.
- Integrate observability with Prometheus and Micrometer.
